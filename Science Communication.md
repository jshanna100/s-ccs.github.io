@def title = "Blogs"
@def tags = [ "Lab"]

# Real-Sci twitter threads

## Table of contents
--------------------------------

<!--ts-->
   * Lab Setup
   * EEG
   * Eye Rotation
   * Vision
   
<!--te-->


### üñ•Ô∏è Lab Setup [![](https://img.shields.io/twitter/url/https/twitter.com/bukotsunikki.svg?style=social&label=Follow)](https://twitter.com/realsci_DE/status/1438037110347476992)

--------------------------------
The neurolab setup in the [@iaostuttgart](https://twitter.com/iaostuttgart). The set-up consists of a box in front of the display, a screen, and a keyboard. The box, which is an extremely quick and precise infrared camera (up to 2000hz), is the eye-tracker.

@@science-communication
[![](../assets/img/monitor.jpg)](../assets/img/monitor.jpg) [![](../assets/img/eye-tracker.jpg)](../assets/img/eye-tracker.jpg) [![](../assets/img/electrode.jpg)](../assets/img/electrode.jpg) 

*Monitor and eye-tracker setup with electode cap*
@@


We can see the infrared illumination extremely clearly when the eye tracker is turned on, but only through the phone camera because it lacks filters, and the human eye only sees a very faint red. The electrodes' connection to the scalp is sound. If not, the entire region will glow red. The gel for the electrodes has been used. After everything is finished, the electrode cap is coupled and calibrated with the eye tracker.



<!-- [https://twitter.com/realsci_DE/status/1438090714320187396](https://twitter.com/realsci_DE/status/1438090714320187396) -->




### üß† EEG [![](https://img.shields.io/twitter/url/https/twitter.com/bukotsunikki.svg?style=social&label=Follow)](https://twitter.com/realsci_DE/status/1437718990927896576)
--------------------------------

Mobile EEG takes research out of the lab and into the real world, sparking interest at the [@NeuroergoConf](https://twitter.com/NeuroergoConf) conference. It isn't particularly modern.

@@science-communication
[![](../assets/img/mobile_eeg.jpg)](../assets/img/mobile_eeg.jpg) 
@@

Because we still move ourüëÄ, mobile EEG is incredibly challenging in practice.

That is when the research actually begins. Developing statistical analytic methods that enable the combination of EEG and eye movements. How much do eye movements influence steady state EEG confounding factors, if at all?

The subject must "fixate" on a spot in practically all studies (98%) and attempt to make no or minimal eye movements.


<!-- [https://twitter.com/realsci_DE/status/1437718990927896576](https://twitter.com/realsci_DE/status/1437718990927896576) -->

### üëÄ Eye Rotation [![](https://img.shields.io/twitter/url/https/twitter.com/bukotsunikki.svg?style=social&label=Follow)](https://twitter.com/realsci_DE/status/1437366798551339008)
--------------------------------

The focus now is on rotating eyeüëÄ!

How many times a day do we move our eyes? So we move our eye about 200.000/day, 3-4 per second - do you also know in which directions? The logo is left-right, up-down. Squinting is another option, but there are also concealed eye motions.

Amazing: Notice how the eye "wobbles" near the conclusion of the motion? The lens is dangling there from its suspension within the eye.


@@science-communication
[![](../assets/img/eye_movement.gif)](../assets/img/eye_movement.gif) 

*Eye movement*
@@

However, we can also rotate our eyes. When we turn our heads in the direction of our shoulders, the eye tries to compensate by stabilizing the image of the environment for as long as possible.

Changes in pupil size are another type of eye movement (in the broadest sense).


<!-- [https://twitter.com/realsci_DE/status/1437366798551339008](https://twitter.com/realsci_DE/status/1437366798551339008) -->


### üëÅÔ∏è Vision [![](https://img.shields.io/twitter/url/https/twitter.com/bukotsunikki.svg?style=social&label=Follow)](https://twitter.com/realsci_DE/status/1438438680927084549)
--------------------------------

Because we can only see well "in focus" and not out of the corner of our eyes, we move our eyes. In the corner of your eye, how does your world appear? According to the survey, 16% of respondents can only see "full black and white," 33% can only see "pure colors," and 51% can only see muted colours.

As a result, the peripheral truly displays color (test below)! Sadly, the myth that there are much more rods (rods, black vision, B/W) than cones prevent us from seeing color there (cones, color vision).

@@science-communication
[![](../assets/img/color_periphery.jpg)](../assets/img/color_periphery.jpg) [![](../assets/img/color_periphery_2.jpg)](../assets/img/color_periphery_2.jpg)

*We can see color in the periphery*
@@

There is a second consequence, though: in the "focus" (fovea), there is a 1:1 link, whereas at the "corner of the eye" (periphery), several channels are bundled together, making one more sensitive but also less accurate. Because of this, the fovea is overrepresented in the brain, and peripheral stimuli must be much more intense for humans to notice them.

Going back to color, it turns out that we can see color even as well in the periphery if we linearly magnify the images and incorporate this "Cortical Magnification Factor." If we don't include it (few horizontal color circles), we don't. 

Now test it for yourself by placing your nose close to the screen:-)

@@science-communication
[![](../assets/img/test_color.jpg)](../assets/img/test_color.jpg)
@@

<!-- [https://twitter.com/realsci_DE/status/1438438680927084549](https://twitter.com/realsci_DE/status/1438438680927084549) -->



### Installation 
============

# &#9889; Usage [![](https://img.shields.io/twitter/url/https/twitter.com/bukotsunikki.svg?style=social&label=Follow)](https://twitter.com/realsci_DE/status/1438438680927084549)
=====

Linux (manual installation)